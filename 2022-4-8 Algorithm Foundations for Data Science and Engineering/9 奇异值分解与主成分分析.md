在分类任务中,随着维度的升高，分类器的性能会先增加后降低
![[Pasted image 20220407221612.png]]
这可能是维度过高导致过拟合。


- 对于矩阵$A , B \in R ^ { n \times n }$，若存在$n \times n$的可逆矩阵$P$,使得$A = PBP^{-1}$,则称A，B是相似的。
- **矩阵相似**描述了一种矩阵分解方法，它表明一个线性变换可以分解成三个步骤：如果矩阵相似于一个对角矩阵$\Sigma$，矩阵A对应的线性变换首先将空间中的向量经过变换$P_{-1}$，变换后的向量再通过对角矩阵$\Sigma$进行伸缩变换，最后再经过$P$进行一个逆变换。
- 正交矩阵是一类特殊的线性变换，它是保持原点不动，长度不变的旋转变换。
- 若存在对角阵$\Sigma$，正交矩阵$P$,使得$A =P\Sigma P^{T}$，则称$A$是可正交对角化的。
- **正交对角化**也是一种矩阵分解方法，该方法又被称为**特征值分解**。若矩阵A是可正交对角化的，意味着矩阵A对应的线性变换可以分解成三步:将空间向量经过正交矩阵$P^{T}$进行旋转变换，随后通过对角矩阵$\Sigma$进行伸缩变换，最后再经过正交矩阵$P$进行旋转变换，其中$P$和$P^{T}$互为逆变换。

### 奇异值分解
上面介绍了针对**实对称矩阵的特征值分解**，而且正交化仅限于方阵。那么对任意$m \times n$的矩阵，能否也能找到一组正交基,使得它经过变换后还是正交基?就是奇异值分解。


令$A$为$m \times n$的矩阵。事实上，矩阵$A$对应的线性变换是将$n$维空间中的向量映射到$k$维空间中。其中$k\leq m$,且$k=Rank(A)$。奇异值分解的目标是在$n$维空间中找到一组正交基，使得经过线性变换$A$之后还是正交的。

设 
$$\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \cdots, \boldsymbol{v}_{n}$$
为 $n$ 维空间中符合条件的一组正交㐞。这组正交基经过线性变换 $\boldsymbol{A}$ 之后被映射成
$$A v_{1}, A v_{2}, \cdots, A v_{n}$$
如果变换之后的向量也两两正交, 即满足
$$
\left(\boldsymbol{A} v_{i}\right)^{\mathrm{T}} \boldsymbol{A} v_{j}=v_{i}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} v_{j}=0
$$
由于矩阵 $\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$ 为 $n \times n$ 的实对称方阵, 因此它是可以正交对角化的, 如果向量 $v_{1}, v_{2}, \cdots, \boldsymbol{v}_{n}$ 为矩阵 $\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$ 的正交基, 那么
$$
\begin{aligned}
\left(\boldsymbol{A} \boldsymbol{v}_{i}\right)^{\mathrm{T}} \boldsymbol{A} \boldsymbol{v}_{j} &=\boldsymbol{v}_{i}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{v}_{j} \\
&=\lambda_{j} \boldsymbol{v}_{i}^{\mathrm{T}} \boldsymbol{v}_{j}=0
\end{aligned}
$$
其中$\lambda_{j}$为矩阵$A^{T}A$的特征值。因此，实对称方阵$A^{T}A$的正交基在经过线性变换$A$之后还是一组正交基，这就是奇异值分解的由来。为了论述奇异值，还需要引入下面两个引理。

**引理9.1**
对于任意矩阵$A$,则$A^{T}A$与$AA^{T}$都是半正定矩阵。
引理9.1表明，实对称矩阵$A^{T}A$与$AA^{T}$都是可以正交对角化的，而且它们的特征值都是非负数的。


**引理 $9.2$** 对于任意矩阵 $\boldsymbol{A}$, 左
$$
\operatorname{rank}\left(A A^{\mathrm{T}}\right)=\operatorname{rank}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)=\operatorname{rank}(\boldsymbol{A})
$$
成立。
引理 $9.2$ 表明, 实对称矩阵 $A^{\mathrm{T}} \boldsymbol{A}$ 与 $\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$ 有 $\operatorname{rank}(\boldsymbol{A})$ 个非零特征值。

**定理 $9.4$** 设 $A \in \mathrm{R}^{m \times n}, \lambda_{i}$ 和 $\mu_{i}$ 分别为矩阵 $A^{\mathrm{T}} A$ 与 $A A^{\mathrm{T}}$ 的特征值，并均为实数。若是特征值 $\lambda_{i}$ 与 $\mu_{i}$ 按照从小到大的顺序排列, 那么
$$
\lambda_{i}=\mu_{i}>0, \quad i=1,2, \cdots, r
$$
其中 $r=\operatorname{rank}(\boldsymbol{A})$ 。
由引理 $9.1$ 和引理 $9.2$, 排序后的特征值㳾足
$$
\begin{aligned}
&\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \lambda_{r}>\lambda_{r+1}=\lambda_{r+2}=\cdots=\lambda_{n}=0 \\
&\mu_{1} \geqslant \mu_{2} \geqslant \cdots \mu_{r}>\mu_{r+1}=\mu_{r+2}=\cdots=\mu_{n}=0
\end{aligned}
$$
定理 $9.4$ 表明, 实对称矩阵 $A^{\mathrm{T}} \boldsymbol{A}$ 和 $\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$ 不仅具有相同个数的非零特征值，而且 其特征值都是相等的。
**定义 $9.5$**   奇异值
设 $A \in \mathrm{R}_{r}^{m \times n}, \lambda_{i}$ 和 $\mu_{i}$ 分别为矩阵 $A^{\mathrm{T}} A$ 与 $A A^{\mathrm{T}}$ 的实特征值, 称
$$
\alpha_{i}=\sqrt{\lambda_{i}}=\sqrt{\mu_{i}}, \quad i=1,2, \cdots, r
$$
是矩阵 $\boldsymbol{A}$ 的正奇异值, 简称奇异值。
结合定理 $9.4$ 和定义 $9.5$ 可以发现, 矩阵 $A$ 的**非零奇异值的个数**等于 $\operatorname{rank}(A)$。

#### 奇异值分解
奇异值分解是将一个$m \times n$的矩阵分解为三个矩阵的乘积，这三个矩阵分别为$m \times m$的方阵、$m \times n$的对角阵和$n \times n$的方阵。
![[Pasted image 20220408183119.png]]
**分解后的矩阵可能存在很多$0$**，为了节约空间，可等价地分解为图9.8所示的三个矩阵相乘，这种分解方式叫做**满秩奇异值分解**。
![[Pasted image 20220408183936.png]]
#### Relation to Eigen-decomposition
- SVD gives us:
- $\boldsymbol{A}=\boldsymbol{U} \Sigma \boldsymbol{V}^{T}$
- Eigen-decomposition:
Shows how to comp
- $\boldsymbol{A}=\boldsymbol{X} \Lambda \boldsymbol{X}^{T}$

	- A is symmetric
	- U, $V, X$ are orthonormal (U'U=I),
	- $\Lambda, \Sigma$ are diagonal
Now let's calculate:
$$
\begin{aligned}
&\mathbf{A} \mathbf{A}^{\top}=\mathbf{U} \Sigma \mathbf{V}^{\top}\left(\mathbf{U} \Sigma \mathbf{V}^{\top}\right)^{\top}=\mathbf{U} \Sigma \mathbf{V}^{\top}\left(\mathbf{V} \Sigma^{\top} \mathbf{U}^{\top}\right)=\mathbf{U} \Sigma \Sigma^{\top} \mathbf{U}^{\top} \\
&\mathbf{A}^{\top} \mathbf{A}=\mathbf{V} \Sigma^{\top} \mathbf{U}^{\top}\left(\mathbf{U} \Sigma \mathbf{V}^{\top}\right)=\mathbf{V} \Sigma^{\top} \mathbf{V}^{\top}
\end{aligned}
$$
因此$U$的列向量为矩阵$AA^{T}$的特征向量，通常称其为左奇异特征向量，而$V$的列向量为矩阵$A^{T}A$
的右奇异特征向量，并且$A$的奇异值是矩阵$A^{T}A$和$AA^{T}$的非零特征值的算术平方根。

基于上述分析，可以得出任意矩阵$A$的奇异值分解过程。具体步骤如下:
1. 计算矩阵$AA^{T}$、$A^{T}A$
2. 分别计算矩阵$AA^{T}$、$A^{T}A$的特征值和对应的特征向量
3. 用矩阵$AA^{T}$的特征向量组成矩阵$U$，矩阵$A^{T}A$的特征向量组成矩阵$V$
4. 对矩阵$AA^{T}$,  $A^{T}A$的非零特征值求出算术平方根，并对应特征向量的位置填入$\Sigma$的对角元。





### SVD:Drawbacks
**Optimal low-rank approximation**
in terms of Frobenius norm 
**Interpretability problem:**
- A singular vector specifies a linear combination of all input columns or rows 
**Lack of sparsity:**
" Singular vectors are **dense**!
![[Pasted image 20220408194242.png]]
UV分解

奇异值分解可以实现维度约减，这种方法称为主成分分析。它仅关注数据特征中的重要特征，而忽略数据中的次要特征甚至是噪声。主成分分析不直接对数据进行奇异值分解，而是**对数据点的协方差矩阵进行奇异值分解**

### 主成分分析
主成分分析 (PCA) 是一种*数据约减或降维技术*。它是一个线性变换, 即把数据变换到一个新的坐标系统中, 使得高维空间中的一个数据点分别投影到第一大方差所在坐标方向 (称为第一主成分) 上、第二大方差所在坐标方向 (第二主成分)上, 依此类推。主成分分析经常用于约减数据集的维数, 同时保留数据集对方差贡献最大的特征。通过保留重要的主成分，忽略不太重要的主成分保留数据的重要特征，从而达到数据维度约减的目的, 即发现高维数据中的主要特征。

给定一组经过中心化的 $m$ 条样本数据 $D=\left(x_{1}, x_{2}, \cdots, x_{m}\right)$, 其中 $\boldsymbol{x}_{i} \in \mathbf{R}^{d}, \sum_{i=1}^{m} \boldsymbol{x}_{i}=$ 0。假设经过投影变换后的新坐标系 $W=\left(w_{1}, w_{2}, \cdots, w_{d^{\prime}}\right)$, 其中 $w_{i} \in \mathbf{R}^{\mathrm{d}}$ 为标准正 $z_{i}=W^{\mathrm{T}} x_{i}$, 最终可基于 $z_{i}$ 来重构原始数据 $x_{i}$, 其**重构**数据 $\widehat{\boldsymbol{x}}_{i}=\boldsymbol{W} \boldsymbol{z}_{\mathrm{i}}$ 。


1. **基于最小重构误差的 PCA 推导**
重构误差度量了维度约减后数据与原始数据间的倫差, 重构误差越小, 意味着维度 约减后数据较好地保留了原始数据的特点, 反之则相反。如图 $9.11$ 所示, 数据点分布在 二维空间的一个椭圆范围内，数据在水平方向上分布比较广，而在坚直方向分㳍比较集 中。无论将数据点投影到水平或坚直方向都可以实现数据降维,但降维后的数据和原数 据间存在一定的误差。比较向水平方向投影利向坚直方向投影，数据被投影到水平方向偏差会更小。换句话说，通过原始数据投影到水平方向比投影到竖直方向信息量的损失少。
![[9.1.png]]

假设 $\mathrm{PCA}$ 对应的线性变换为 $W$, 则变换后的数据点为 $\widehat{\boldsymbol{x}}_{i}=\boldsymbol{W} \boldsymbol{x}_{i}$ 。若采用欧式距离作为距离度量, 则所有重构样本点与原样本之间的距离为
$$
\begin{aligned}
\sum_{i=1}^{m}\left\|\widehat{x}_{i}-\boldsymbol{x}_{i}\right\|_{2}^{2} &=\sum_{i=1}^{m}\left\|\boldsymbol{W} \boldsymbol{z}_{i}-\boldsymbol{x}_{i}\right\|_{2}^{2} \\
&=\sum_{i=1}^{m}\left(\left(W \boldsymbol{z}_{i}\right)^{\mathrm{T}}\left(\boldsymbol{W} \boldsymbol{z}_{i}\right)-2\left(\boldsymbol{W} \boldsymbol{z}_{i}\right)^{\mathrm{T}} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{i}\right) \\
&=\sum_{i=1}^{m}\left(\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{W}^{\mathrm{T}} \boldsymbol{W} \boldsymbol{z}_{i}-2 z_{i}^{\mathrm{T}} \boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{i}\right)
\end{aligned}
$$
注意到 $W$ 为正交基组成的矩阵, 所以 $W^{\mathrm{T}} W=I$ 。因此, 重构误差可以改写为
$$
\begin{aligned}
\sum_{i=1}^{m}\left(z_{i}^{\mathrm{T}} \boldsymbol{W}^{\mathrm{T}} W z_{i}-2 z_{i}^{\mathrm{T}} \boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{i}+\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{i}\right) &=\sum_{i=1}^{m}\left(\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{i}-2 \boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{i}+\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{i}\right) \\
&=-\sum_{i=1}^{m}\left(\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{i}-\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{i}\right) \\
&=-\operatorname{tr}\left(\boldsymbol{z}_{i} z_{i}^{\mathrm{T}}\right)+\text { const } \\
&=-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right)+\text { const }
\end{aligned}
$$
由此可得, 最小化重构误差的目标函数为
$$
\begin{aligned}
&\min _{W}-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right) \\
&\text { 约束条件 } \boldsymbol{W}^{\mathrm{T}} \boldsymbol{W}=1
\end{aligned}
$$
用拉格叻日乘子法求上式的最优解，得
$$
\boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} W=\lambda W
$$
主要到$X$是中心化的数据，所以$XX^{T}$为原始数据的协方差矩阵。因此，将求得的特征值按照从大到小排序$\lambda_{1} \geq\lambda_{2}\geq \dots\geq\lambda_{d}$，再取前$k$个特征值对应的特征向量构成新的低维坐标系 $W=\left(w_{1}, w_{2}, \cdots, w_{k}\right)$, 其中 $k < d$，即主成分分析的解。

2. **基于最大可分性的 $\mathrm{PCA}$ 推导**
可分性度量了样本点 $x_{i}$ 任投影到新空间后的离化样本点在新的空问中的方差。观察图 $9.11$ 可以发现, 将样本点投影到水平方向上时 数据之问分得比䢂开, 但投影到柽直方向上时大部分样本点重难在一起, 极端情况下可能退化到一个点。因此, 水平方向更适合作为最优的投影方向。
投影后的样本点为 $W^{\mathrm{T}} x_{i}$, 那么***所有样本点在新空间中在各维度的方差总和***为
$$
\sum_{i=1}^{m}\left(W^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{\mathrm{T}}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{x}_{i}\right)=\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right)
$$
由此可发现, **最大化投影后的样本点的方差等同于优化如下目标函数:**
$$
\min _{\boldsymbol{W}}-\operatorname{tr}\left(\boldsymbol{W}^{\mathrm{T}} \boldsymbol{X} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{W}\right)
$$
约束条件 $W^{\top} \boldsymbol{W}=1$
它与基于**最小化重构误差**推导出的 PCA 优化目标函数是一致的。

3. **PCA 的算法实现**
根据 PCA 的推导容易发现, 主成分分布在协方差矩阵正交基的方向上。为了找到 主成分, 仅需确定协方差矩阵的正交基即可, 即对协方差矩阵进行正交对角化。所以，本质上来说, PCA 算法 (其实现如算法 $9.1$ 所示) 是奇异值分解的一个重要应用。
算法 $9.1$ 描述了主成分分析的过程, 其输人是数据矩阵 $X \in \mathrm{R}^{m \times n}$, 由 $m$ 个 $n$ 维 的样本组成。算法主要步骤如下:
(1) 样本点去中心化 (算法第 $1-2$ 行)。令 $\bar{x}=\frac{1}{m} \sum_{j=1}^{m} x_{j}$ 为样本中心点,
$$
\boldsymbol{y}_{i}=\boldsymbol{x}_{i}-\overline{\boldsymbol{x}}, i=1,2, \cdots, m
$$
(2) 样本协方差矩阵的奇异值分解 (算法第 $3-4$ 行)。计算得到的协方差矩阵 $\frac{Y Y^{\mathrm{T}}}{m-1} \in$ $\mathbf{R}^{m \times m}$, 对该协方差矩阵采用 $9.3$ 节中介绍的奇异值分解方法述行分解, 即
$$
\frac{\boldsymbol{Y} \boldsymbol{Y}^{\mathrm{T}}}{m-1}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{U}^{\mathrm{T}}
$$
经过奇异值分解后，矩阵 $U$ 中的列向量可以作为正交基向量。
(3) 主成分选取 (算法第 5 行)。首先将奇异值按照从大到小顺序排列，即
$$
\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{m}
$$
根据以下规则确定主成分的个数 $k$ :
$$
k=\arg \min _{l}\left\{\frac{\sum_{i=1}^{l} \lambda_{i}}{\sum_{i=1}^{\operatorname{rank}(\boldsymbol{A})} \lambda_{i}} \geqslant \alpha\right\}
$$
由前 $k$ 个奇异值所对应的特征向量组成矩阵
$$
\boldsymbol{W}=\left(\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \cdots, \boldsymbol{w}_{k}\right)
$$
这里的 $\alpha$ 为一个阈值, 限定主成分对原始数据的解释能力, 通常 $\alpha$ 设定在 $90 \%$ 左右。
(4) 数据点向新空间中投影 (算法第 6-7 行)。将每个样本点 $\boldsymbol{x}_{i}$ 投影到新空间中, 得到新样本点 $z_{i}=W^{\mathrm{T}} x_{i}$, 其中 $z_{i}$ 和 $x_{i}$ 分别是 $k$ 维和 $n$ 维的, 由于 $k<n$, 从而实现 了数据降维。
(5) 降维输出。降维后的样本集为 $D^{\prime}=\left(z_{1}, z_{2}, \cdots, z_{m}\right)$ 。