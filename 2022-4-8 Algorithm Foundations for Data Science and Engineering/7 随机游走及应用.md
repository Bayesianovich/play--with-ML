- 设 $(\Omega, F,(P)$ 是一个概率空间, $T$表示时间集合, 若对 $t \in T$ ) 均有定义在 $(\Omega, F, P)$ 上的一个随机变量 $X(t, \omega)(\omega \in \Omega)$ 与之对应, 则称依赖于时间 $t$ 的随机变量序列 $X(t, \omega)$ 为一个随机过程记为 $\{X(t, \omega), t \in T, \omega \in \Omega\}$, 简记为 $\{X(t), t \in T\}$ 
- 特别地, 随机过程 $\{X(t), t \in T\}$ 所有可能的取值称为该随机过程的状态空间或 值域。
- 随机过程 $\{X(t, \omega), t \in T, \omega \in \Omega\}$ 是与时间 $t$ 有关的一个随机变量序列, 现实生活中有很多问题都可以利用随机过程来建模. 比如文本数据、语音数据、股票价格和用户上网行为等。

**Joint Probability:**
The r.v.s $X_{1}, \cdots, X_{n}$ are a sequence of discrete r.v.s, then joint $P\left(X_{1}=x_{1}, \cdots, X_{n}=x_{n}\right)$ can be computed as
- If $X_{1}, \cdots, X_{n}$ are mutually independent r.v.s, $\prod_{i=1}^{n} P\left(X_{i}=x_{i})\right.$
- $P\left(X_{1}=x_{1}\right) \prod_{i=2}^{n} P\left(X_{i}=x_{i} \mid X_{i-1}=x_{i-1}, \cdots, X_{1}=x_{1}\right)$.


**Remarks:**
- When r.v.s are dependent, the generative model is hard to **scalable** to large scale applications;
- We need to **simplify the model in practise.**
- The **first-order** correlation in language model can be defined as:
$$
P\left(X_{i}=x_{i} \mid X_{i-1}=x_{i-1}, \cdots, X_{1}=x_{1}\right)=P\left(X_{i}=x_{i} \mid X_{i-1}=x_{i-1}\right) .
$$
- For **financial applications**, the stock price can be modeled by $t$-order correlation $P\left(X_{i}=x_{i} \mid X_{i-1}=x_{i-1}, \cdots, X_{1}=x_{1}\right)=$ $P\left(X_{i}=x_{i} \mid X_{i-1}=x_{i-1} \ldots, X_{i-t}=X_{i-t}\right)$


Discrete time Markov Chain
A stochastic process $X_{0}, X_{1}, \cdots$ of discrete time and discrete space is a Markov chain if it satisfies the Markov property, i.e.,
$$
P\left(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}, \cdots, X_{0}=x_{0}\right)=P\left(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}\right)
$$
for all $n$ and all $x_{i} \in \Omega$.
- A Markov chain $\left\{X_{t}\right\}$ is said to be**time homogeneous**（时间齐性） if $P\left(X_{s+t}=j \mid X_{s}=i\right)$ is independent of $s$. When this holds, putting $s=0$ gives
$$
P\left(X_{s+t}=j \mid X_{s}=i\right)=P\left(X_{t}=j \mid X_{0}=i\right)
$$
If moreover $P ( X _ { n + 1 } = j | X _ { n } = i ) = P _ { i j }$  is independent of n，then X is said to be time homogeneous Markov chain.

**Probability transition matrix**
Definition：
Let a Markov chain have $P_{x, y}^{(t+1)}=P\left[X_{t+1}=y \mid X_{t}=x\right]$, and the finite state space be $\Omega=[n]$. This gives us a **probability transition matrix** $P^{(t+1)}$ at time $t$. The probability transition matrix is an $n \times n$ matrix of nonnegative entries such that**the sum over each row** of $P^{(t)}$ is 1 , since $\forall n$ and $\forall x_{i} \in \Omega$
$$
\sum_{y} P_{x, y}^{(t+1)}=\sum_{y} P\left[X_{t+1}=y \mid X_{t}=x\right]=\frac{\sum_{y} P\left[X_{t+1}=y, X_{t}=x\right]}{P\left[X_{t}=x\right]}=1
$$
**State distribution**
Definition：
Let $\pi^{(t)}$ be the **state distribution** of the chain at time $t$, that $\pi_{x}^{(t)}=$ $P\left[X_{t}=x\right]$.
For a **finite chain**, $\pi^{(t)}$ is a vector of $n$ nonnegative entries such that $\sum_{x} \pi_{x}^{(t)}=1$. Then, it holds that $\pi^{(t+1)}=\pi^{(t)} P^{(t+1)}$. We apply the law of total probability.
$$
\begin{aligned}
&\pi_{y}^{(t+1)} \\
&=\sum_{x} P\left[X_{t+1}=y \mid X_{t}=x\right] P\left[X_{t}=x\right] \\
&=\sum_{x} \pi_{x}^{(t)} P_{x, y}^{(t+1)} \\
&=\left(\pi^{(t)} P^{(t+1)}\right)_{y}
\end{aligned}
$$
**Stationary distributions**
Definition:
A stationary distribution of a finite Markov chain with transition matrix $P$ is a probability distribution $\pi$ such that $\pi P=\pi$.
- For some Markov chains, no matter what the initial distribution is, after running the chain for a while, the distribution of the chain approaches the stationary distribution.
- 
- E.g., $P^{20}=\left(\begin{array}{cccc}0.4 & 0.6 & 0 & 0 \\ 0.4 & 0.6 & 0 & 0 \\ 0 & 0 & 0.5 & 0.5 \\ 0 & 0 & 0.5 & 0.5\end{array}\right)$. The chain could converge to any distribution which is a linear combination of $(0.4,0.6,0,0)$ and $(0,0,0.5,0.5)$. We observe that the original chain $P$ can be broken into two disjoint Markov chains, which have their own stationary distributions. We say that the chain is reducible.（可约的，可简化的）。
**Irreducibility**
State $y$ is **accessible from** state $x$ if it is possible for the chain to visit state $y$ if the chain starts in state $x$, in other words, $P^{n}(x, y)>0, \forall n$. State $x$ **communicates with** state $y$ if $y$ is accessible from $x$ and $x$ is accessible from $y$. **We say that the Markov chain is irreducible if all pairs of states communicates.**
- $y$ is accessible from $x$ means that $y$ is connected from $x$ in the transition graph, i.e., there is a directed path from $x$ to $y$
- $x$ communicates with $y$ means that $x$ and $y$ are strongly connected in the transition graph
- A finite Markov chain is irreducible if and only if its transition graph is strongly connected
- The Markov chain with transition matrix $P$ is not irreducible
- The communication relation satisfies reflexive, symmetric, and transitive

![[irreducibility.png]]
**Period**
The **period** of a state $x$ is the **greatest common divisor(最大公约数)** (gcd), such that $d_{x}=\operatorname{gcd}\left\{n \mid\left(P^{n}\right)_{x, x}>0\right\}$.
If $\forall n \geq 1,\left(P^{n}\right)_{x, x}=0$, then $d_{x}=\infty$.
- For example, suppose that the period of state $x$ is $d_{x}=3$.
Then, starting from state $x$, chain $x, \bigcirc, \bigcirc, \square, \bigcirc, \bigcirc, \square, \cdots$, only the squares are possible to be $x$.
- In the transition graph of a finite Markov chain, $\left(P^{n}\right)_{x, x}>0$  i.e.,  equivalent to that $x$ is on a cycle of length $n$.
- Period of a state $x$ is the greatest common devisor of the lengths of cycles passing $x$.

A state is **aperiodic** if its period is 1 . A Markov chain is *aperiodic* if **all its states are aperiodic.**

**Properties of period**
1. If **the states $x$ and $y$ communicate, then $d_{x}=d_{y}$**. Consequently, if the Markov chain is irreducible, then all states have the same period.
2. We have $\left(P^{n}\right)_{x, x}=0$ if $n\left(\bmod d_{x}\right) \neq 0$.
![[eg.png]]

### Ranking vertices on the graph
Page is more important if it has more links.

##### Intuition
- Web pages are important if people visit them a lot.
- But we cannot watch everybody using the Web.
- A good surrogate for visiting pages is to assume people follow links randomly.
- Leads to random surfer model:
	- Start at a random page and follow random outlinks repeatedly ,from whatever page you are at (state transition).
	- PageRank = limiting probability of being at a page (stationary distribution).
	- **Solve the recursive equation**: "importance of a page $=$ its sha of the importance of each of its predecessor pages" (Equivalent to the random-surfer definition of PageRank)
- The problem is formulated to **compute the stationary distribution of the Web graph**.
