- 考虑到资源的消耗，往往精确计算是得不尝失的，随机算法是常用的方法。随机算法作为一种近似方法，需要在理论上保证算法给出的近似值是可以接受的。在这种情况下，尾概率不等式是一个不可或缺的工具。

**Markov inequality**
If $X$ is any Ir.v. and $0<a<+\infty$, then
$$
P(X>a) \leq \frac{E(X)}{a} \text { or } P(X>a E(X)) \leq \frac{1}{a}
$$

Proof：
$$P ( X \gt a ) = \int _ { x \gt a } d x \leq \int \frac { x } { a } d x = \frac { E ( X ) } { a  }$$

**Chebyshevs inequality**
If r.v. $X$ has mean and variance $\mu=E(X)$ and $\sigma^{2}=E\left[(X-\mu)^{2}\right]$, then
$$
P(|X-\mu|>a) \leq \frac{\sigma^{2}}{a^{2}} \text { or } P(|X-\mu|>a E(X)) \leq \frac{\sigma^{2}}{a^{2} E(X)^{2}}
$$
**Chernoff bound**
Theorem
Let $X_{i}$ be a sequence of *independent Bernoulli* r.v.s with $P\left(X_{i}=\right.$ 1) $=p_{i}$. Assume that r.v. $X=\sum_{i=1}^{n} X_{i}$.
- $P(X<(1-\delta) \mu)<\left(\frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^{\mu}$, where $\mu=\sum_{i=1}^{n} p_{i}$
- $P(X<(1-\delta) \mu)<\exp \left(-\mu \delta^{2} / 2\right)$
Proof
For $t>0$,
$$
\begin{aligned}
P(X<(1-\delta) \mu) &=P(\exp (-t X)>\exp (-t(1-\delta) \mu)) \\
&<\frac{\Pi_{i=1}^{n} E\left(\exp \left(-t X_{i}\right)\right)}{\exp (-t(1-\delta) \mu)}(\text { Markov inequality })
\end{aligned}
$$
Since $\left(1-x<e^{-x}\right)$, we have
$$
\begin{aligned}
E\left(\exp \left(-t X_{i}\right)\right) &=p_{i} e^{-t}+\left(1-p_{i}\right)=1-p_{i}\left(1-e^{-t}\right)<\exp \left(p_{i}\left(e^{-t}-1\right)\right) \\
\Pi_{i=1}^{n} E\left(\exp \left(-t X_{i}\right)\right) &<\Pi_{i=1}^{n} \exp \left(p_{i}\left(e^{-t}-1\right)\right)=\exp \left(\mu\left(e^{-t}-1\right)\right)
\end{aligned}
$$
Hence,
$$
\begin{aligned}
P(X<(1-\delta) \mu) &<\frac{\exp \left(\mu\left(e^{-t}-1\right)\right)}{\exp (-t(1-\delta) \mu)} \\
&=\exp \left(\mu\left(e^{(-t)}+t-t \delta-1\right)\right)
\end{aligned}
$$
Now its time to choose $t$ to make the bound as tight as possible. Taking the derivative of $\mu\left(e^{(-t)}+t-t \delta-1\right)$ and setting $-e^{(-t)}+1-\delta=0$. We have $t=\ln (1 / 1-\delta)$.
$$
P(X<(1-\delta) \mu)<\left(\frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^{\mu}
$$

Proof of second statement
To get the simpler form of the bound, we need to get rid of the clumsy term $(1-\delta)^{(1-\delta)}$
$$
\begin{aligned}
(1-\delta) \ln (1-\delta) &=(1-\delta)\left(\sum_{i=1}^{\infty}-\frac{\delta^{i}}{i}\right)>-\delta+\frac{\delta^{2}}{2} \\
(1-\delta)^{(1-\delta)} &>\exp \left(-\delta+\frac{\delta^{2}}{2}\right)
\end{aligned}
$$
Furthermore,
$$
\begin{aligned}
P(X<(1-\delta) \mu) &<\left(\frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^{\mu} \\
&<\left(\frac{e^{-\delta}}{\exp \left(-\delta+\frac{\delta^{2}}{2}\right)}\right)^{\mu} \\
&=\exp \left(-\mu \delta^{2} / 2\right)
\end{aligned}
$$
likely;
Theorem for upper tail
Let $X_{i}$ be a sequence of independent and Bernoulli r.v.s with $P\left(X_{i}=1\right)=p_{i} .$ Assume that r.v. $X=\sum_{i=1}^{n} X_{i}$ and $\mu=$ $\sum_{i=1}^{n} p_{i}$.
- $P(X>(1+\delta) \mu)<\left(\frac{e^{\delta}}{(1+\delta)^{(1+\delta)}}\right)^{\mu}$
- $P(X>(1+\delta) \mu)<\exp \left(-\mu \delta^{2} / 4\right)$


e.g.
Let $X$ be the number of heads in $n$ tosses of a fair coin, then $\mu=\frac{n}{2}$ and $\delta=\frac{1}{2}$, we have
$$
P\left(X>\frac{3 n}{4}\right)=P\left(X>\left(1+\frac{1}{2}\right) \frac{n}{2}\right)<\exp \left(-\frac{n}{2} \delta^{2} / 4\right)=\exp (-n / 32)
$$
If we toss the coin 1000 times, the probability is less than exp $(-125 / 4)$.


### Counting problem
Problem definition
The algorithm must monitor a sequence of events, then at any given time output of the number of events thus far. More formally, this is a data structure maintaining a single integer $n$ and supporting the following two operations:
*update()*:  increments $n$ by 1 ;
*query ()* : must output (an estimate of ) ${n}$.

**Morris algorithm (Morris 1978)**
1 : initialize $X \leftarrow 1$;
2: for each update, increment $X$ with probability $\frac{1}{2^{X}}$;
3: for a query, output $\widehat{n}=2^{X}-1$.



![[morris algorithm.pdf]]







**Tug of War**(拔河)
这种综合运用Chebyshev不等式和Chernoff不等式得到一个相对紧的概率上界的方法称为Tug of War.
It is a simple technique to reduce the dependence on the failure probability $\delta$ from to log $1/  \delta$.
这项技术被广泛用于随机算法误差分析中。
Chebyshev不等式是以多项式的方式不断收紧，而Chernoff不等式给出的概率上界以幂的方式不断收缩。Hoeffding不等式可以对有界随机变量给出于Chernoff不等式类似的尾概率上界。

尾概率不等式具有广泛的应用，比如随机算法分析，概率的近似求解和负载均衡等。