_I.e._ stands for _id est_, which means "that is."

 _E.g._ stands for _exempli gratia_, which means "for example.

 网络通常可以看作是一个随机游走，随机游走的平稳分布可以可以看作是网络中顶点的中心性(centrality)度量。而随机游走的平稳分布实际上是其概率转移矩阵特征值为1所对应的特征向量。
 
 递归(recursion)是重复调用函数自身实现循环 (A调用A)；迭代(iteration)是函数内某段代码实现循环(A 重复调用B)
 
 高斯消元法计算低阶方阵的特征值和特征向量还能接受，但对于高阶$100 \times 100$方阵，这种效率太低。因此高斯消元法不具有可扩展性，需要找到快速、高效的特征值和特征向量计算方法。

- 幂法和反幂法是迭代求解实矩阵特征值和特征向量的方法。具体来说，幂法是计算矩阵规模最大的特征值和对应特征向量计算方法。

## Power method
定理 $8.3$ 设 $\boldsymbol{A} \in \mathbf{R}^{n \times n}$ 有 $n$ 个线性无关的特征向量。矩阵 $\boldsymbol{A}$ 的特征值为 ${\lambda_{1}, \lambda_{2}, \cdots,}$ $\lambda_{n}$, 与之对应的特征向量为 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \cdots, \boldsymbol{p}_{n}$, 其中 $\boldsymbol{p}_{i} \in \mathbf{R}^{n}(i=1,2, \cdots, n)$ 。已知 $\boldsymbol{A}$ 的 主特征值 $\lambda_{1}$ 是实根, 且满足条件
$$\left|\lambda_{1}\right|>\left|\lambda_{2}\right| \geqslant\left|\lambda_{3}\right| \geqslant \cdots \geqslant\left|\lambda_{n}\right|$$
定义 $\boldsymbol{v}_{0}=\sum_{i=1}^{n} \alpha_{i} \boldsymbol{p}_{i} \neq \mathbf{0}\left(\alpha_{1} \neq 0\right), \underline{\boldsymbol{v}_{k}}=\boldsymbol{A} \boldsymbol{v}_{k-1}(k=1,2, \cdots)$, 则
$$\lim _{k \rightarrow \infty} \frac{\boldsymbol{v}_{k}}{\lambda_{1}^{k}}=\alpha_{1} \boldsymbol{p}_{i}$$
$$\lim _{k \rightarrow \infty} \frac{\left(\boldsymbol{v}_{k+1}\right)_{i}}{\left(\boldsymbol{v}_{k}\right)_{i}}=\lambda_{1}$$
根据定理 8.3,
$$
\boldsymbol{v}_{k}=\boldsymbol{A} \boldsymbol{v}_{k-1}=\boldsymbol{A}^{2} \boldsymbol{v}_{k-2}=\cdots \equiv \boldsymbol{A}^{k} \boldsymbol{v}_{0}
$$
因此, 求解$\boldsymbol{v}_{k}$ 相当于求方阵 $\boldsymbol{A}$的幂运算，这就是幂法的由来。


**Proof:**

设实矩阵 $A=\left(a_{i j}\right)$ 有 $n$ 个线性无关的特佂向量, 且矩阵 $A$ 的特佂值为 $\lambda_{1}, \lambda_{2}, \cdots$, $\lambda_{n}$, 与之对应的特征向量为 $p_{1}, p_{2}, \cdots, p_{n}$, 其中 $p_{i} \in \mathbf{R}^{n}(i=1,2, \cdots, n)$ 。 已知 $A$ 的主特征值$\lambda_{1}$ 是实数且不是重根, 即满足条件
$$
\left|\lambda_{1}\right|>\left|\lambda_{2}\right| \geqslant\left|\lambda_{3}\right| \geqslant \cdots \geqslant\left|\lambda_{n}\right|
$$
其中, $\lambda_{1}$ 称为矩阵 $A$ 的主特征值。满足上述条件的方阵, 可以运用幂法求解主特征值 $\lambda_{1}$ 及其对应的特征向量 $p_{1}$ 。
令 $v_{0}$ 为非零的初始向量, 它可以由正交基 $\boldsymbol{p}_{1}, \boldsymbol{p}_{2}, \cdots, \boldsymbol{p}_{\mathrm{n}}$ 线性表出为
$$
v_{0}=\alpha_{1} p_{1}+\alpha_{2} p_{2}+\cdots+\alpha_{n} p_{n}
$$
利用矩阵 $A$ 的乘幂构造如下的向量序列:
$$
\left\{\begin{array}{l}
v_{1}=A v_{0}=\sum_{i=1}^{n} \alpha_{i} \lambda_{i} \boldsymbol{p}_{i } \\
v_{2}=A v_{1}=A^{2} v_{0}=\sum_{i=1}^{n} \alpha_{i} \lambda_{i}^{2} \boldsymbol{p}_{i} \\
\cdots \\
v_{k+1}=A v_{k}=A^{k+1} v_{0}=\sum_{i=1}^{n} \alpha_{i} \lambda_{i}^{k+1} \boldsymbol{p}_{i} \\
\cdots
\end{array}\right.
$$
其中 $v_{k}(k=1,2, \cdots)$ 为第 $k$ 轮的迭代向量。因此,
$$v_{k}=A v_{k-1}=A^{k} v_{0}=\alpha_{1} \lambda_{1}^{k} p_{1}+\alpha_{2} \lambda_{2}^{k} p_{2}+\cdots+\alpha_{n} \lambda_{n}^{k} p_{n}$$
$$=\lambda_{1}^{k}\left[\alpha_{1} p_{1}+\sum_{i=2}^{n} \alpha_{i}\left(\lambda_{i} / \lambda_{1}\right)^{k} p_{i}\right]=\lambda_{1}^{k}\left(\alpha_{1} p_{1}+\epsilon_{k}\right)$$
其中 $\epsilon_{k}=\sum_{i=2}^{n} \alpha_{i}\left(\lambda_{i} / \lambda_{1}\right)^{k} p_{i}$ 由于 $\lambda_{1}$ 为主待征值, 所以 $\left|\lambda_{i} / \lambda_{1}\right|<1(i=2,3, \cdots, n)$ 。 那么 $\lim _{k \rightarrow \infty} \epsilon_{k}=0$, 进而可得
$$
\lim _{k \rightarrow \infty} \frac{v_{k}}{\lambda_{1}^{k}}=\alpha_{1} \boldsymbol{p}_{1}
$$
因此, 当 $k$ 充分大时, $v_{k} / \lambda_{1}^{k}$ 越来越接近向量 $\alpha_{1} \boldsymbol{p}_{10}$ 换言之, 迭代次数足侈多时
$$
v_{k} \approx \alpha_{1} \lambda_{1}^{k} \boldsymbol{p}_{1}
$$
即迭代向量 $v_{k}$ 几乎平行于特征值 $\lambda_{1}$ 对应的特佂向量 $p_{1}$ (除了相差一个常数因子)。



关于主特征值 $\lambda_{1}$ 的计算, 可先构造
$$
\frac{\left(\boldsymbol{v}_{k+1}\right)_{i}}{\left(\boldsymbol{v}_{k}\right)_{i}}=\lambda_{1}\left\{\frac{\alpha_{1}\left(\boldsymbol{p}_{1}\right)_{i}+\left(\boldsymbol{\epsilon}_{k+1}\right)_{i}}{\alpha_{1}\left(\boldsymbol{p}_{1}\right)_{i}+\left(\boldsymbol{\epsilon}_{k}\right)_{i}}\right\}
$$
两边取极限,
$$
\lim _{k \rightarrow \infty} \frac{\left(\boldsymbol{v}_{k+1}\right)_{i}}{\left(\boldsymbol{v}_{k}\right)_{i}}=\lambda_{1}
$$
两个相邻迭代向量对应分量的比值收敛到主特征值。

- 上面计算矩阵特征值的方法称为幂法 该方法通过非零向量 $v_{0}$ 及矩阵 $A$ 的乘 幂 $\boldsymbol{A}^{k}$ 来构造向量序列 $\left\{\boldsymbol{v}_{k}\right\}(k=1,2, \cdots)$, 进一步计算 $\boldsymbol{A}$ 的主特征值 $\lambda_{1}$ 和 相应的特征向量。
上面算法需要改进,规范化向量(normalized vector) 有效避免溢出现象。

Assume that for a matrix $A$ there is a unique (ie only one) largest eigenvalue $\lambda_{1}$, i.e.,
$$
\lambda_{1}=\max _{i} \lambda_{i}
$$
Then we can find $\lambda_{1}$ by the Power method.
- The power method is an iterative algorithm which has the following basic form for generating a single eigenvalue and eigenvector of $A$.
1: Pick a starting vector $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, such that $\left\|\mathbf{x}^{(0)}\right\|=1$;
2: $\quad$ For $k=1,2, \cdots$;
3: $\quad$ Let $\mathbf{v}=A \mathbf{x}^{(k-1)}$;
4: $\quad$ Let $\mathbf{x}^{(k)}=\frac{\mathbf{v}}{\|\mathbf{v}\|}$;
It terminates when some convergence conditions are satisfied.

**The power method Extension**
Inverse power method: 
it operates with $A^{-1}$ rather than $A$ since the eigenvalues of $A^{-1}$ are $\frac{1}{\lambda_{i}}$. It gives a way of finding the smallest (in absolute value) eigenvalue of a matrix.
**Inverse iteration Cont'd**

The advantage of inverse iteration is that it can be **easily adapted to find any eigenvalue of the matrix $A$, instead of just the extreme ones.**
- Observe that for any $\mu \in R$, the matrix $B=A-\mu I$ has eigenvalues $\left\{\lambda_{1}-\mu, \cdots, \lambda_{n}-\mu\right\}$. **In particular, by choosing $\mu$ to be close to an eigenvalue $\lambda_{j}$ of $A$,** we can ensure that $\lambda_{j}-\mu$ is the eigenvalue of $B$ of smallest magnitude.

1.Pick some $\mu$ close the desired eigenvalue;
2: Pick a starting vector $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, such that $\left\|\mathbf{x}^{(0)}\right\|=1$;
3:  For $k=1,2, \cdots$;
4: $\quad$ Solve $(A-\mu I) \mathbf{v}=\mathbf{x}^{(k-1)}$ for $\mathbf{v}$;
5: $\quad$ Let $\mathbf{x}^{(k)}=\frac{\mathbf{v}}{\|\mathbf{v}\|}$;


**Newton iteration**
Using linear Approximations to solve equations $f(x)=0$, where $f(x)$ is a well-behaved function (differentiable). Let $r$ be a root of the equation $f(x)=0$.
- Each iteration of the Newton iteration returns an approximate and improved root.
1: Pick a starting point $x_{0}$ ;
2: Until the convergence condition ;
4: $\quad x_{k+1}=x_{k}-\frac{f\left(x_{k}\right)}{f^{\prime}\left(x_{k}\right)}$ ;
- Since the true root is $r$, and $h=r-x_{0}$, the number $h$ measures how far the estimate $x_{0}$ is from the truth.
牛顿迭代法使用可导函数$f(x)$的泰勒级数的前两项来寻找方程$f(x)=0$的近似解。
![[Pasted image 20220408121810.png]]
Why does Rayleigh  Quotient work？

A good way to understand Rayleigh quotient iteration is as a sort of Newton iteration for the eigenvalue equations.
Write
$$
F(\mathbf{v}, \lambda)=A \mathbf{v}-\lambda \mathbf{v}=0,
$$
and differentiate to find
$\Delta F=(A-\lambda /) \Delta \mathbf{v}-(\Delta \lambda) \mathbf{v}$, Recall: $\triangle z=\frac{\partial z}{\partial x} \Delta x+\frac{\partial z}{\partial y} \Delta y .$
- Newton iteration gives
$$
\begin{aligned}
0 &=F\left(\mathbf{v}_{k}, \lambda_{k}\right)+\triangle F\left(\mathbf{v}_{k}, \lambda_{k}\right) \\
&=\left(A-\lambda_{k} I\right)\left(\mathbf{v}_{k}+\Delta \mathbf{v}_{k}\right)-\left(\triangle \lambda_{k}\right) \mathbf{v}_{k} \\
&=\left(A-\lambda_{k} I\right) \mathbf{v}_{k+1}-\left(\Delta \lambda_{k}\right) \mathbf{v}_{k},
\end{aligned}
$$
从而得到新的近似解为$v_{k+1}=\left(\Delta \lambda_{k}\right)\left(A-\lambda_{k} I\right)^{-1} \mathbf{v}_{k}$，其中$\left(\Delta \lambda_{k}\right)$为规范化因子
**Rayleigh Quotient method**
The inverse iteration method can be improved if we drop the restriction that the shift value remains constant throughout the iterations.
- Each iteration of the shifted inverse iteration method returns an approximate eigenvector, given an estimate of an eigenvalue.
1: Pick a starting vector $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, such that $\left\|\mathbf{x}^{(0)}\right\|=1$;
2: Let $\lambda^{(0)}=r\left(\mathbf{x}^{(0)}\right)=\left(\mathbf{x}^{(0)}\right)^{T} A \mathbf{x}^{(0)}$;
3:For  $k=1,2, \cdots$;
	4	    Solve $\left(A-\lambda^{(k-1)} /\right) \mathbf{v}=\mathbf{x}^{(k-1)}$ for $\mathbf{v}$;
	5: $\quad$ Let $\mathbf{x}^{(k)}=\frac{v}{\|\mathbf{v}\|}$;
	6: $\quad$ Let $\lambda^{(k)}=r\left(\mathbf{x}^{(k)}\right)=\left(\mathbf{x}^{(k)}\right)^{T} A \mathbf{x}^{(k)}$;
- The method is only guaranteed to converge when the matrix $A$ is**both real and symmetric,** and is known to fail in the casses where the matrix is not symmetric.
- **Analysis of Rayleigh Quotient method**
Following the previous analysis, considering only the two largest eigenvalues,
$R_{A}\left(\mathbf{x}^{(k)}\right)=\frac{\mathbf{x}^{(k)^{T}} A \mathbf{x}^{(k)}}{\mathbf{x}^{(k)^{T}} \mathbf{x}^{(k)}} \approx \frac{\left(c_{1} \lambda_{1}^{k} \mathbf{v}_{1}+c_{2} \lambda_{2}^{k} \mathbf{v}_{2}\right)^{T}\left(c_{1} \lambda_{1}^{k+1} \mathbf{v}_{1}+c_{2} \lambda_{2}^{k+1} \mathbf{v}_{2}\right)}{\left(c_{1} \lambda_{1}^{k} \mathbf{v}_{1}+c_{2} \lambda_{2}^{k} \mathbf{v}_{2}\right)^{T}\left(c_{1} \lambda_{1}^{k} \mathbf{v}_{1}+c_{2} \lambda_{2}^{k} \mathbf{v}_{2}\right)}$
$=\frac{c_{1}^{2} \lambda_{1}^{2 k+1}+c_{2}^{2} \lambda_{2}^{2 k+1}}{c_{1}^{2} \lambda_{1}^{2 k}+c_{2}^{2} \lambda_{2}^{2 k}}=\lambda_{1} \frac{1+\frac{c_{2}^{2}}{c_{1}^{2}}\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{2 k+1}}{1+\frac{c_{2}^{2}}{c_{1}^{2}}\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{2 k}}$
- The proportional error thus decays with successive iterations as $\left|\frac{\lambda_{2}}{\lambda_{1}}\right|^{2 k}$.
- It gives us a locally quadratically convergent algorithm, i.e.$\left|\frac{\lambda_{2}}{\lambda_{1}}\right|^{2 }$.

**Performance comparison**
![[Pasted image 20220408164527.png]]
**Motivation of deflation technique**
- So far we have considered how to find the largest eigenvalue. Suppose we have done this, then how do we find the rest? After finding the largest eigenvalue $\lambda_{1}$, a more general way forward is to **make it into the smallest by deflation** and then go on to find the new largest one, say $\lambda_{2}$.
- We can repeatedly find all eigenvalues of a symmetric matrix $A$ with distinct eigenvalues because
$$
A \doteq A^{(0)}=\sum_{i=1}^{n} \lambda_{i} \mathbf{v}_{i} \mathbf{v}_{i}^{T}
$$
and
$$
A^{(i)}=A^{(i-1)}-\lambda_{i} \mathbf{v}_{i} \mathbf{v_{i}}^{\top} .
$$
**	Analysis of deflation technique**

Consider
$$
\left(A-\lambda_{1} \mathbf{v}_{1} \mathbf{v}_{1}^{T}\right) \mathbf{v}_{j}=A \mathbf{v}_{j}-\lambda_{1} \mathbf{v}_{1} \mathbf{v}_{1}^{T} \mathbf{v}_{j}
$$
We have
$$
\left(A-\lambda_{1} \mathbf{v}_{1} \mathbf{v}_{1}^{T}\right) \mathbf{v}_{j}= \begin{cases}0 \cdot \mathbf{v}_{j}, & \text { if } j=1 \\ \lambda_{j} \cdot \mathbf{v}_{j}, & \text { otherwise }\end{cases}
$$
- Thus, $A-\lambda_{1} \mathbf{v}_{1} \mathbf{v}_{1}^{T}$ has the same eigenvectors as $A$, and the same eigenvalues as $A$ except that the largest one has been replaced by 0 .
- We can find the similar conclusion for $A^{(i-1)}-\lambda_{i} \mathbf{v}_{i} \mathbf{v}_{i}^{T}$.
- Thus, we can use the power method with Rayleigh's coefficient to find the next biggest and so on.
- 如果$\lambda_2$不出现重根，从而可以利用幂法求得矩阵$A_{1}$ 的主特征值和主特征向量