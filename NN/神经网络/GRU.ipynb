{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "\n",
    "import  torch\n",
    "import  torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "from    torch.autograd import Variable\n",
    "\n",
    "import  torchvision.datasets as dsets\n",
    "import  torchvision.transforms as trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_set = dsets.MNIST(root = '../data/',transform=trans.ToTensor(),train=True,download=True)\n",
    "test_set = dsets.MNIST(root='../data/',transform=trans.ToTensor(),train=False)\n",
    "train_dl = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,num_classes):\n",
    "        super(GRU,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.feature = nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        device = x.device\n",
    "        h0 = Variable(torch.zeros(self.num_layers,x.size(0),self.hidden_size)).to(device)\n",
    "        out,_ = self.feature(x,h0)\n",
    "        out = self.classifier(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 200\n",
    "seq_len = 28\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "nepochs = 30\n",
    "\n",
    "net = GRU(input_size,hidden_size,num_layers,num_classes)\n",
    "if torch.backends.mps.is_available():\n",
    "    net = net.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,criterion,dataloader):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.view(-1, seq_len, input_size)\n",
    "        batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        if torch.backends.mps.is_available():\n",
    "            batch_x = batch_x.to('mps')\n",
    "            batch_y = batch_y.to('mps')\n",
    "        logits = model(batch_x)\n",
    "        error = criterion(logits, batch_y)\n",
    "        loss += error.item()\n",
    "        \n",
    "        probs, pred_y = logits.data.max(dim=1)\n",
    "        accuracy += (pred_y == batch_y.data).sum()/batch_y.size(0)\n",
    "        \n",
    "    loss /= len(dataloader)\n",
    "    accuracy = accuracy*100/len(dataloader)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/30, 14 sec|\t train loss: 0.6914, train acc: 77.02 |\t  test loss: 0.6830, test acc: 77.61\n",
      " 2/30, 13 sec|\t train loss: 0.3220, train acc: 90.13 |\t  test loss: 0.3108, test acc: 90.45\n",
      " 3/30, 13 sec|\t train loss: 0.1811, train acc: 94.52 |\t  test loss: 0.1787, test acc: 94.98\n",
      " 4/30, 13 sec|\t train loss: 0.1511, train acc: 95.50 |\t  test loss: 0.1559, test acc: 95.67\n",
      " 5/30, 13 sec|\t train loss: 0.1025, train acc: 96.92 |\t  test loss: 0.1066, test acc: 96.77\n",
      " 6/30, 13 sec|\t train loss: 0.0842, train acc: 97.43 |\t  test loss: 0.0866, test acc: 97.44\n",
      " 7/30, 13 sec|\t train loss: 0.0702, train acc: 97.87 |\t  test loss: 0.0786, test acc: 97.53\n",
      " 8/30, 13 sec|\t train loss: 0.0687, train acc: 97.91 |\t  test loss: 0.0754, test acc: 97.68\n",
      " 9/30, 13 sec|\t train loss: 0.0480, train acc: 98.59 |\t  test loss: 0.0599, test acc: 98.10\n",
      "10/30, 13 sec|\t train loss: 0.0429, train acc: 98.69 |\t  test loss: 0.0545, test acc: 98.33\n",
      "11/30, 13 sec|\t train loss: 0.0416, train acc: 98.70 |\t  test loss: 0.0547, test acc: 98.34\n",
      "12/30, 13 sec|\t train loss: 0.0318, train acc: 99.05 |\t  test loss: 0.0480, test acc: 98.57\n",
      "13/30, 13 sec|\t train loss: 0.0341, train acc: 98.98 |\t  test loss: 0.0455, test acc: 98.58\n",
      "14/30, 13 sec|\t train loss: 0.0274, train acc: 99.16 |\t  test loss: 0.0435, test acc: 98.53\n",
      "15/30, 13 sec|\t train loss: 0.0247, train acc: 99.25 |\t  test loss: 0.0450, test acc: 98.60\n",
      "16/30, 13 sec|\t train loss: 0.0222, train acc: 99.35 |\t  test loss: 0.0424, test acc: 98.70\n",
      "17/30, 13 sec|\t train loss: 0.0171, train acc: 99.51 |\t  test loss: 0.0338, test acc: 98.90\n",
      "18/30, 13 sec|\t train loss: 0.0218, train acc: 99.32 |\t  test loss: 0.0433, test acc: 98.66\n",
      "19/30, 13 sec|\t train loss: 0.0157, train acc: 99.53 |\t  test loss: 0.0392, test acc: 98.71\n",
      "20/30, 13 sec|\t train loss: 0.0164, train acc: 99.48 |\t  test loss: 0.0383, test acc: 98.83\n",
      "21/30, 13 sec|\t train loss: 0.0130, train acc: 99.62 |\t  test loss: 0.0381, test acc: 98.91\n",
      "22/30, 13 sec|\t train loss: 0.0160, train acc: 99.52 |\t  test loss: 0.0415, test acc: 98.78\n",
      "23/30, 13 sec|\t train loss: 0.0108, train acc: 99.68 |\t  test loss: 0.0391, test acc: 98.92\n",
      "24/30, 13 sec|\t train loss: 0.0079, train acc: 99.79 |\t  test loss: 0.0340, test acc: 98.93\n",
      "25/30, 13 sec|\t train loss: 0.0083, train acc: 99.78 |\t  test loss: 0.0364, test acc: 98.95\n",
      "26/30, 14 sec|\t train loss: 0.0093, train acc: 99.77 |\t  test loss: 0.0379, test acc: 98.88\n",
      "27/30, 13 sec|\t train loss: 0.0064, train acc: 99.85 |\t  test loss: 0.0373, test acc: 98.95\n",
      "28/30, 13 sec|\t train loss: 0.0064, train acc: 99.83 |\t  test loss: 0.0339, test acc: 99.00\n",
      "29/30, 13 sec|\t train loss: 0.0100, train acc: 99.68 |\t  test loss: 0.0443, test acc: 98.75\n",
      "30/30, 13 sec|\t train loss: 0.0064, train acc: 99.83 |\t  test loss: 0.0387, test acc: 98.91\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nepochs):\n",
    "    since = time.time()\n",
    "    for batch_x,batch_y in train_dl:\n",
    "        batch_x = batch_x.view(-1, seq_len, input_size)\n",
    "        batch_y = Variable(batch_y)\n",
    "        if torch.backends.mps.is_available():\n",
    "            batch_x = batch_x.to('mps')\n",
    "            batch_y = batch_y.to('mps')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        logits = net(batch_x)\n",
    "        error = criterion(logits,batch_y)\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    now = time.time()\n",
    "    train_loss,train_acc = eval(net,criterion,train_dl)\n",
    "    test_loss,test_acc = eval(net,criterion,test_dl)\n",
    "    print('%2d/%d, %.0f sec|\\t train loss: %.4f, train acc: %.2f |\\t  test loss: %.4f, test acc: %.2f' % (epoch+1,nepochs,now-since,train_loss,train_acc,test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
