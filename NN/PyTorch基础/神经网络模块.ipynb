{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn中提供搭建神经网络所需模块，而这些模块的具体在torch.nn.functional中实现\n",
    "- Layer函数,包括:线性变换、卷积变化、池化、批归一化(Batch Normalization)、Dropout等\n",
    "- 激活函数:ReLU、Sigmoid、Tanh、LeakyReLU、Softmax等\n",
    "- 损失函数:均方误差、交叉熵、负对数似然、Hinge等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3])\n",
      "tensor([[-0.1603,  0.7083, -0.0668],\n",
      "        [-0.0050,  1.0916, -0.3441],\n",
      "        [-0.2729,  0.9737, -0.1249],\n",
      "        [-0.2514,  1.2877, -0.2878],\n",
      "        [-0.1480,  1.4313, -0.4192],\n",
      "        [-0.2719,  1.0551, -0.1643],\n",
      "        [-0.1481,  0.8691, -0.1509]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(2,3)\n",
    "x = torch.rand(7,2)\n",
    "y = net(x)\n",
    "\n",
    "print(y.shape)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[-0.4011,  0.5458],\n",
      "        [ 0.5547,  0.5251],\n",
      "        [-0.0207, -0.5824]], requires_grad=True) \n",
      "\n",
      "bias Parameter containing:\n",
      "tensor([-0.2882,  0.5677,  0.0781], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, para in net.named_parameters():\n",
    "    print(name, para,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4011,  0.5458],\n",
      "        [ 0.5547,  0.5251],\n",
      "        [-0.0207, -0.5824]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.2882,  0.5677,  0.0781], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in net.parameters():\n",
    "    print(para,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.4011,  0.5458],\n",
      "        [ 0.5547,  0.5251],\n",
      "        [-0.0207, -0.5824]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2882,  0.5677,  0.0781], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "paras = list(net.parameters())\n",
    "print(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True) \n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], grad_fn=<AddmmBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.weight.data = torch.zeros(3,2)\n",
    "print(net.weight,'\\n')\n",
    "\n",
    "net.bias.data = torch.ones(3)\n",
    "print(net.bias,'\\n')\n",
    "\n",
    "y = net(x)\n",
    "print(y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.rand(2,2)\n",
    "    y = net(x)\n",
    "    print(y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5320, grad_fn=<DotBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,requires_grad=False)\n",
    "y = torch.rand(2,requires_grad=True)\n",
    "z = torch.dot(x,y)\n",
    "print(z,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6670, 0.2001]) \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(y.grad,'\\n')\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10, 28, 28]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv2d(5,10,kernel_size=3,padding=1)\n",
    "x = torch.rand(7,5,28,28)\n",
    "y = net(x)\n",
    "\n",
    "print(y.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10, 14, 14]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv2d(5,10,kernel_size=3,padding=1,stride=2)\n",
    "x = torch.rand(7,5,28,28)\n",
    "y = net(x)\n",
    "\n",
    "print(y.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 28, 28]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv2d(2,5,kernel_size=5,padding=2,stride=1)\n",
    "x = torch.rand(7,2,28,28)\n",
    "y = net(x)\n",
    "\n",
    "print(y.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([5, 2, 5, 5]) \n",
      "\n",
      "bias torch.Size([5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, para in net.named_parameters():\n",
    "    print(name, para.shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3, 3]) \n",
      "\n",
      "torch.Size([2, 3, 3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3,9,9)\n",
    "avg_pool = nn.AvgPool2d(kernel_size=3)\n",
    "max_pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "y1 = avg_pool(x)\n",
    "y2 = max_pool(x)\n",
    "\n",
    "print(y1.data.shape,'\\n')\n",
    "print(y2.data.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4]) \n",
      "\n",
      "torch.Size([2, 3, 4, 4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3,9,9)\n",
    "avg_pool = nn.AvgPool2d(kernel_size=3,stride=2)\n",
    "max_pool = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "\n",
    "y1 = avg_pool(x)\n",
    "y2 = max_pool(x)\n",
    "\n",
    "print(y1.data.shape,'\\n')\n",
    "print(y2.data.shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.8172, 0.9634],\n",
      "        [0.0000, 0.5376, 0.7961],\n",
      "        [0.0000, 1.1626, 0.0000]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "relu1 = nn.ReLU(inplace=True)\n",
    "x = torch.randn(3,3)\n",
    "y = relu1(x)\n",
    "print(y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.8172, 0.9634],\n",
      "        [0.0000, 0.5376, 0.7961],\n",
      "        [0.0000, 1.1626, 0.0000]]) True\n"
     ]
    }
   ],
   "source": [
    "print(x, y is x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6273, 0.2712, 0.0999],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]]) \n",
      "\n",
      "tensor([[ 0.6273,  0.2712,  0.0999],\n",
      "        [-1.1771, -0.8340, -0.5153],\n",
      "        [-0.2561, -0.2042, -0.2384]]) False\n"
     ]
    }
   ],
   "source": [
    "relu2 = nn.ReLU()\n",
    "x = torch.randn(3,3)\n",
    "y = relu2(x)\n",
    "\n",
    "print(y,'\\n')\n",
    "print(x, y is x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(relu1.inplace,relu2.inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tanh = nn.Tanh()\n",
    "x = torch.rand(3,3)*1000\n",
    "\n",
    "y = tanh(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "d = 20\n",
    "\n",
    "x1 = torch.ones(B,d)\n",
    "x2 = -torch.ones(B,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "z = loss(x1,x2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(320.)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss(reduction='sum')\n",
    "z = loss(x1, x2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "         4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "         4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "         4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "         4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss(reduction='none')\n",
    "z = loss(x1, x2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module \n",
    "- torch.nn中的每个模块都是继承自nn.Module。譬如下面代码片段实现全连接层\n",
    "- nn.Module中的forward()函数是实现前向传播的函数，backward()函数是实现反向传播的函数\n",
    "```python\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "```\n",
    "继承:\n",
    "- nn.Module中的forward()函数是实现前向传播的函数，backward()函数是实现反向传播的函数\n",
    "\n",
    "- zero_grad()函数用于梯度清零\n",
    "\n",
    "- parameters()函数用于返回模型参数\n",
    "\n",
    "- to()函数用于将模型转移到GPU上\n",
    "\n",
    "- state_dict()函数用于返回模型参数字典\n",
    "\n",
    "- load_state_dict()函数用于加载模型参数字典\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadActivationFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadActivationFunc, self).__init__() \n",
    "        a = torch.tensor(1).float()   \n",
    "        self.a = nn.Parameter(a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.a * x * x\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "acfun = QuadActivationFunc()\n",
    "x = torch.tensor([1.,2.],requires_grad=True)\n",
    "y = acfun(x)\n",
    "S = y.sum()\n",
    "S.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(S.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acfun.a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Functional\n",
    "- nn中还有一个很常用的模块:nn.functional,nn中的大多数layer,在functional中都能找到与之对应的函数。nn.Module与nn.functional的区别在于，nn.Module实现的layer是一个特殊的类，会自动提取可学习参数nn.Parameter,而nn.functional是一个函数库，由def.function定义。nn.Module中的layer都是类的形式，需要实例化后才能使用，而nn.functional中的函数都是函数，可以直接调用。当函数中不存在可学习的参数时，nn.Module与nn.functional的功能是一样的，但是当函数中存在可学习参数时，nn.Module的功能更加强大，因为它能够提取出可学习参数，而nn.functional就显得无能为力了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6710, -0.6160, -0.4166], grad_fn=<ViewBackward0>) \n",
      "\n",
      "tensor([-0.6710, -0.6160, -0.4166], grad_fn=<ViewBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = nn.Linear(2,3)\n",
    "x = torch.rand(2)\n",
    "output1 = f(x)\n",
    "\n",
    "output2 = F.linear(x, f.weight, f.bias)\n",
    "\n",
    "print(output1,'\\n')\n",
    "print(output2,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, m, n):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n, m))\n",
    "        self.b = nn.Parameter(torch.randn(n))\n",
    "        self.c = torch.zeros(3)\n",
    "        \n",
    "    def initalize(self,a,b):\n",
    "        self.W.uniform_(a,b)\n",
    "        self.b.zero_()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        o = x.matmul(self.W.t()) + self.b\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Linear(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W Parameter containing:\n",
      "tensor([[ 0.3538, -0.4064, -0.4798],\n",
      "        [-0.6161, -0.2097, -1.9453]], requires_grad=True) \n",
      "\n",
      "b Parameter containing:\n",
      "tensor([-0.1072, -0.2490], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, para in f.named_parameters():\n",
    "    print(name, para,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<SubBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,3)\n",
    "y = f(x)\n",
    "y1 = f.forward(x)\n",
    "\n",
    "print(y1-y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1756, -0.9457, -1.1510],\n",
      "        [-6.0586, -4.8742, -5.9472]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,3)\n",
    "y = f(x)\n",
    "e = y.pow(2).sum()\n",
    "e.backward()\n",
    "print(f.W.grad,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f.zero_grad()\n",
    "print(f.W.grad,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[-0.9096, -0.5388,  1.2883],\n",
       "                      [ 0.1372, -0.5839, -0.0594]])),\n",
       "             ('b', tensor([-0.2127,  0.0490]))])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Linear(3,2)\n",
    "f.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(f.state_dict(),'linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Tensor.ipynb             optim.ipynb               y_new.pkl\n",
      "2Autograd.ipynb           x.pkl                     神经网络模块.ipynb\n",
      "linear.pt                 y.pkl\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Linear(3,2)\n",
    "g.load_state_dict(torch.load('linear.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.W == f.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 定义一个线性层\n",
    "linear_layer = nn.Linear(10, 5)\n",
    "\n",
    "# 使用 kaiming_normal_ 初始化权重\n",
    "init.kaiming_normal_(linear_layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# 将偏差初始化为零\n",
    "init.constant_(linear_layer.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
